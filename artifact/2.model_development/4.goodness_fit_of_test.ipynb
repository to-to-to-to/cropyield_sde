{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform OU and GBM process fitting and rank-based GOF test\n",
    "def perform_ou_gbm_rank_gof_test(df, yield_column):\n",
    "    # Extract the YIELD column for testing\n",
    "    yield_data = df[yield_column].dropna()\n",
    "\n",
    "    # Step 1: Fitting the Ornstein-Uhlenbeck (OU) process\n",
    "    mu_ou = np.mean(yield_data)  # Long-term mean set to the mean of the actual data\n",
    "    theta_ou = 0.15  # Mean reversion rate (how fast the process reverts to the mean)\n",
    "    sigma_ou = 0.1  # Volatility\n",
    "    T = len(yield_data)  # Number of time points from the actual data\n",
    "\n",
    "    # Simulate OU Process for trend\n",
    "    ou_trend = np.zeros(T)\n",
    "    ou_trend[0] = yield_data.iloc[0]  # Start from the first actual data point\n",
    "    for t in range(1, T):\n",
    "        ou_trend[t] = ou_trend[t - 1] + theta_ou * (mu_ou - ou_trend[t - 1]) + sigma_ou * np.random.normal()\n",
    "\n",
    "    # Step 2: Fitting the Geometric Brownian Motion (GBM) process\n",
    "    mu_gbm = 0.02  # Drift\n",
    "    sigma_gbm = 0.1  # Volatility\n",
    "\n",
    "    gbm_trend = np.zeros(T)\n",
    "    gbm_trend[0] = yield_data.iloc[0]\n",
    "    for t in range(1, T):\n",
    "        gbm_trend[t] = gbm_trend[t - 1] * np.exp((mu_gbm - 0.5 * sigma_gbm ** 2) + sigma_gbm * np.random.normal())\n",
    "\n",
    "    # Combine OU and GBM into a 2D array for comparison at each time step\n",
    "    simulated_data = np.vstack((ou_trend, gbm_trend))\n",
    "\n",
    "    # Step 3: Calculate ranks for the observed data compared to the simulated data\n",
    "    ranks = np.zeros(T)\n",
    "\n",
    "    # For each time step, rank the observed value compared to the simulated values\n",
    "    for t in range(T):\n",
    "        combined_simulations = np.append(simulated_data[:, t], yield_data[t])  # Include the observed value\n",
    "        sorted_simulations = np.sort(combined_simulations)  # Sort them\n",
    "        ranks[t] = np.where(sorted_simulations == yield_data[t])[0][0] + 1  # Find the rank of the observed value\n",
    "\n",
    "    # Step 4: Calculate the test statistic based on the rank frequencies\n",
    "    M = simulated_data.shape[0] + 1  # Total number of simulated values + 1 for observed\n",
    "    expected_frequency = (M + 1) / 2  # Expected rank is the middle of the distribution\n",
    "\n",
    "    # Chi-square test statistic\n",
    "    chi_square_stat = np.sum((ranks - expected_frequency) ** 2 / expected_frequency)\n",
    "\n",
    "    # Degrees of freedom\n",
    "    df = T - 1  # One degree of freedom for each time point minus one\n",
    "    p_value_rank = 1 - chi2.cdf(chi_square_stat, df)\n",
    "\n",
    "    # Output the results\n",
    "    rank_gof_result = {\n",
    "        'Chi-Square Statistic': chi_square_stat,\n",
    "        'P-Value': p_value_rank,\n",
    "        'Result': \"Reject H0\" if p_value_rank < 0.05 else \"Fail to reject H0\"\n",
    "    }\n",
    "\n",
    "    print(rank_gof_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "file_path_nuts0 = '../datasets/CropSDEData/YIELD_NUTS0_NL.csv'\n",
    "file_path_nuts2 = '../datasets/CropSDEData/YIELD_NUTS2_NL_transposed.csv'  # Updated NUTS2 dataset\n",
    "file_path_mcyfs = '../datasets/CropSDEData/YIELD_PRED_MCYFS_NUTS0_NL.csv'\n",
    "\n",
    "nuts0_df = pd.read_csv(file_path_nuts0)\n",
    "nuts2_df = pd.read_csv(file_path_nuts2)\n",
    "mcyfs_df = pd.read_csv(file_path_mcyfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NUTS0 Dataset (National Level) ---\n",
      "{'Chi-Square Statistic': 53.5, 'P-Value': 1.0, 'Result': 'Fail to reject H0'}\n"
     ]
    }
   ],
   "source": [
    "# Run tests for NUTS0 dataset\n",
    "print(\"\\n--- NUTS0 Dataset (National Level) ---\")\n",
    "perform_ou_gbm_rank_gof_test(nuts0_df, yield_column='YIELD')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NUTS2 Dataset (Regional Level) ---\n",
      "{'Chi-Square Statistic': 348.0, 'P-Value': 1.0, 'Result': 'Fail to reject H0'}\n"
     ]
    }
   ],
   "source": [
    "# Run tests for NUTS2 dataset\n",
    "print(\"\\n--- NUTS2 Dataset (Regional Level) ---\")\n",
    "perform_ou_gbm_rank_gof_test(nuts2_df, yield_column='yield')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MCYFS Predicted Data ---\n",
      "{'Chi-Square Statistic': 147.5, 'P-Value': 1.0, 'Result': 'Fail to reject H0'}\n"
     ]
    }
   ],
   "source": [
    "# Run tests for MCYFS Predicted Data\n",
    "print(\"\\n--- MCYFS Predicted Data ---\")\n",
    "perform_ou_gbm_rank_gof_test(mcyfs_df, yield_column='YIELD_PRED')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the result of the Goodness of Fit (GOF) test is to fail to reject ð»0, it means that the model's predictions are consistent with the observed data, according to the statistical test.\n",
    "\n",
    "- Null Hypothesis (ð»0): The model fits the observed data well. In this case, the combined Ornstein-Uhlenbeck process and Geometric Brownian Motion fit the actual crop yield data.\n",
    "- Fail to Reject ð»0: The statistical test does not find enough evidence to conclude that the model is a poor fit. Therefore, the model seems to fit the data reasonably well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
