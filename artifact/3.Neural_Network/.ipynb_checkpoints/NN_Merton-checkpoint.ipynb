{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f62016-e3a3-431d-a4da-9108441b8351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mk/qm568t1n2t11r_13jw7_kn400000gn/T/ipykernel_21842/236525405.py:68: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df['merton_variance'].iloc[0] = max(df['returns'].var(), 1e-6)\n",
      "/var/folders/mk/qm568t1n2t11r_13jw7_kn400000gn/T/ipykernel_21842/236525405.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['merton_variance'].iloc[0] = max(df['returns'].var(), 1e-6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 - Loss: 0.2831600841078976\n",
      "Epoch 200 - Loss: 0.26660845156980173\n",
      "Epoch 300 - Loss: 0.26354253829425256\n",
      "Epoch 400 - Loss: 0.262591475990305\n",
      "Epoch 500 - Loss: 0.2622406512730146\n",
      "Epoch 600 - Loss: 0.26196047749549994\n",
      "Epoch 700 - Loss: 0.26168635150931724\n",
      "Epoch 800 - Loss: 0.2614179543151859\n",
      "Epoch 900 - Loss: 0.2611549885610726\n",
      "Epoch 1000 - Loss: 0.2608971770149848\n",
      "Epoch 1100 - Loss: 0.2606442611445147\n",
      "Epoch 1200 - Loss: 0.2603959997958952\n",
      "Epoch 1300 - Loss: 0.26015216796579915\n",
      "Epoch 1400 - Loss: 0.2599125556595421\n",
      "Epoch 1500 - Loss: 0.2596769668297695\n",
      "Epoch 1600 - Loss: 0.2594452183900879\n",
      "Epoch 1700 - Loss: 0.25921713929847084\n",
      "Epoch 1800 - Loss: 0.2589925697056053\n",
      "Epoch 1900 - Loss: 0.25877136016367197\n",
      "Epoch 2000 - Loss: 0.25855337089134856\n",
      "Epoch 2100 - Loss: 0.25833847109111063\n",
      "Epoch 2200 - Loss: 0.2581265383151681\n",
      "Epoch 2300 - Loss: 0.257917457876624\n",
      "Epoch 2400 - Loss: 0.25771112230267346\n",
      "Epoch 2500 - Loss: 0.25750743082687816\n",
      "Epoch 2600 - Loss: 0.2573062889177561\n",
      "Epoch 2700 - Loss: 0.2571076078411137\n",
      "Epoch 2800 - Loss: 0.2569113042537275\n",
      "Epoch 2900 - Loss: 0.2567172998261466\n",
      "Epoch 3000 - Loss: 0.25652552089254244\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = len(hidden_sizes) + 1\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        for i in range(1, self.num_layers + 1):\n",
    "            self.weights.append(np.random.randn(sizes[i], sizes[i - 1]) * np.sqrt(2 / sizes[i - 1]))\n",
    "            self.biases.append(np.zeros((sizes[i], 1)))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.activations = [X]\n",
    "        self.z = []\n",
    "        for i in range(self.num_layers):\n",
    "            z = np.dot(self.weights[i], self.activations[i]) + self.biases[i]\n",
    "            self.z.append(z)\n",
    "            if i < self.num_layers - 1:\n",
    "                a = np.tanh(z)  # Tanh for hidden layers\n",
    "            else:\n",
    "                a = z  # Linear activation for output\n",
    "            self.activations.append(a)\n",
    "        return self.activations[-1]\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        m = X.shape[1]\n",
    "        gradients = []\n",
    "        dZ = self.activations[-1] - y\n",
    "        for i in range(self.num_layers - 1, -1, -1):\n",
    "            dW = (1 / m) * np.dot(dZ, self.activations[i].T)\n",
    "            db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            gradients.append((dW, db))\n",
    "            if i > 0:\n",
    "                dA = np.dot(self.weights[i].T, dZ)\n",
    "                dZ = dA * (1 - np.tanh(self.z[i - 1]) ** 2)  # Derivative of tanh\n",
    "        return gradients[::-1]\n",
    "\n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "        for i in range(self.num_layers):\n",
    "            self.weights[i] -= learning_rate * gradients[i][0]\n",
    "            self.biases[i] -= learning_rate * gradients[i][1]\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"../datasets/CropSDEData/METEO_DEKADS_NUTS2_NL.csv\")\n",
    "df = df.dropna(subset=['PREC', 'VPRES', 'WSPD', 'RELH'])\n",
    "\n",
    "# Use estimated parameters from Merton Model\n",
    "mu_hat = -0.006374441588979891\n",
    "sigma_hat = 0.7458928682802284\n",
    "lambda_hat = 0.10250856908232264\n",
    "delta_hat = 0.06891821761948404\n",
    "\n",
    "# Compute log-returns of PREC using log1p (better stability)\n",
    "df['log_PREC'] = np.log1p(df['PREC'])\n",
    "df['returns'] = df['log_PREC'].diff().dropna()\n",
    "df = df.dropna(subset=['returns'])\n",
    "\n",
    "# Simulate variance using Merton Jump-Diffusion Model\n",
    "df['merton_variance'] = np.nan\n",
    "df['merton_variance'].iloc[0] = max(df['returns'].var(), 1e-6)\n",
    "\n",
    "np.random.seed(42)\n",
    "for t in range(1, len(df)):\n",
    "    vt = max(df['merton_variance'].iloc[t - 1], 1e-6)  # Ensure non-negative variance\n",
    "    epsilon = np.random.normal(0, 1)\n",
    "    \n",
    "    # Simulate Jump occurrence\n",
    "    jump_occurred = np.random.poisson(lambda_hat) > 0\n",
    "    jump_size = np.random.normal(0, delta_hat) if jump_occurred else 0\n",
    "    \n",
    "    vt_new = vt + mu_hat + sigma_hat * np.sqrt(vt) * epsilon + jump_size\n",
    "    df.loc[df.index[t], 'merton_variance'] = max(vt_new, 1e-6)\n",
    "\n",
    "# Feature Engineering: Add rolling mean & std deviation\n",
    "df['rolling_mean_PREC'] = df['PREC'].rolling(window=30).mean()\n",
    "df['rolling_std_PREC'] = df['PREC'].rolling(window=30).std()\n",
    "df = df.dropna(subset=['rolling_mean_PREC', 'rolling_std_PREC'])\n",
    "\n",
    "# Define Input (X) and Target (y)\n",
    "X = df[['VPRES', 'WSPD', 'RELH', 'merton_variance', 'rolling_mean_PREC', 'rolling_std_PREC']].values\n",
    "y = np.log1p(df['PREC'].values).reshape(-1, 1)  # Log-transform target for stability\n",
    "\n",
    "# Split Data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "X_train_mean = X_train.mean(axis=0)\n",
    "X_train_std = X_train.std(axis=0)\n",
    "X_train = (X_train - X_train_mean) / X_train_std\n",
    "X_val = (X_val - X_train_mean) / X_train_std\n",
    "\n",
    "# Define MLP model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_sizes = [128, 64, 32]  # Increased hidden layers\n",
    "output_size = y_train.shape[1]\n",
    "mlp = MLP(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 5000\n",
    "learning_rate = 0.01\n",
    "min_learning_rate = 0.0001\n",
    "learning_rate_decay = 0.99\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "early_stopping_threshold = 100\n",
    "patience = 0\n",
    "\n",
    "# Training loop with adaptive learning rate\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = mlp.forward(X_train.T)\n",
    "    gradients = mlp.backward(X_train.T, y_train.T)\n",
    "    mlp.update_parameters(gradients, learning_rate)\n",
    "    \n",
    "    loss = np.mean((outputs - y_train.T) ** 2)\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1} - Loss: {loss}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    if patience > early_stopping_threshold:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "    # Decay learning rate\n",
    "    learning_rate = max(min_learning_rate, learning_rate * learning_rate_decay)\n",
    "\n",
    "# Testing\n",
    "test_outputs = mlp.forward(X_val.T)\n",
    "test_loss = np.mean((test_outputs - y_val.T) ** 2)\n",
    "\n",
    "# Convert back to original scale\n",
    "test_outputs_original = np.expm1(test_outputs.T)  # Reverse log-transform\n",
    "y_val_original = np.expm1(y_val)\n",
    "\n",
    "final_mse = np.mean((test_outputs_original - y_val_original) ** 2)\n",
    "print(\"Final Test MSE (Original Scale):\", final_mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b642e97-95f8-46a3-ae83-382d4c2a136c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
