{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67bae0dc-cbff-4642-8fc9-e9533b4c4a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/var/folders/mk/qm568t1n2t11r_13jw7_kn400000gn/T/ipykernel_22960/4189320736.py:69: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df['heston_variance'].iloc[0] = df['variance'].iloc[0]\n",
      "/var/folders/mk/qm568t1n2t11r_13jw7_kn400000gn/T/ipykernel_22960/4189320736.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['heston_variance'].iloc[0] = df['variance'].iloc[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 - Loss: 2.0105736591265617\n",
      "Epoch 200 - Loss: 1.501867897638782\n",
      "Epoch 300 - Loss: 1.2362626204983362\n",
      "Epoch 400 - Loss: 1.0603066539658819\n",
      "Epoch 500 - Loss: 0.9231415940188732\n",
      "Epoch 600 - Loss: 0.8062567352070794\n",
      "Epoch 700 - Loss: 0.7024210930235351\n",
      "Epoch 800 - Loss: 0.6089245095139749\n",
      "Epoch 900 - Loss: 0.5251471918621335\n",
      "Epoch 1000 - Loss: 0.4513145419182812\n",
      "Epoch 1100 - Loss: 0.38765680051129564\n",
      "Epoch 1200 - Loss: 0.333928829848703\n",
      "Epoch 1300 - Loss: 0.2893280712389621\n",
      "Epoch 1400 - Loss: 0.2526838289054455\n",
      "Epoch 1500 - Loss: 0.2227112041260545\n",
      "Epoch 1600 - Loss: 0.1981970863072942\n",
      "Epoch 1700 - Loss: 0.17809028105761743\n",
      "Epoch 1800 - Loss: 0.16152217006379743\n",
      "Epoch 1900 - Loss: 0.14779209213643776\n",
      "Epoch 2000 - Loss: 0.13634079635464352\n",
      "Epoch 2100 - Loss: 0.1267234958420397\n",
      "Epoch 2200 - Loss: 0.1185866308766275\n",
      "Epoch 2300 - Loss: 0.11164900260339793\n",
      "Epoch 2400 - Loss: 0.1056867541214694\n",
      "Epoch 2500 - Loss: 0.10052146116095045\n",
      "Epoch 2600 - Loss: 0.09601068523998854\n",
      "Epoch 2700 - Loss: 0.09204047942083471\n",
      "Epoch 2800 - Loss: 0.08851944968355131\n",
      "Epoch 2900 - Loss: 0.08537405789176941\n",
      "Epoch 3000 - Loss: 0.08254491412147033\n",
      "Epoch 3100 - Loss: 0.07998385450936786\n",
      "Epoch 3200 - Loss: 0.07765164017925114\n",
      "Epoch 3300 - Loss: 0.0755161453996206\n",
      "Epoch 3400 - Loss: 0.073550930039656\n",
      "Epoch 3500 - Loss: 0.07173411338310715\n",
      "Epoch 3600 - Loss: 0.07004748410348813\n",
      "Epoch 3700 - Loss: 0.06847579535093987\n",
      "Epoch 3800 - Loss: 0.06700620507165607\n",
      "Epoch 3900 - Loss: 0.06562783043796953\n",
      "Epoch 4000 - Loss: 0.0643313920995386\n",
      "Epoch 4100 - Loss: 0.06310892928193562\n",
      "Epoch 4200 - Loss: 0.06195357089032277\n",
      "Epoch 4300 - Loss: 0.060859350987205005\n",
      "Epoch 4400 - Loss: 0.05982105951197234\n",
      "Epoch 4500 - Loss: 0.05883412105739583\n",
      "Epoch 4600 - Loss: 0.05789449603899516\n",
      "Epoch 4700 - Loss: 0.05699859978324899\n",
      "Epoch 4800 - Loss: 0.05614323599381044\n",
      "Epoch 4900 - Loss: 0.05532554178812107\n",
      "Epoch 5000 - Loss: 0.054542942073979854\n",
      "Final Test MSE: 0.06103336211721661\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = len(hidden_sizes) + 1\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        for i in range(1, self.num_layers + 1):\n",
    "            self.weights.append(np.random.randn(sizes[i], sizes[i - 1]) * np.sqrt(2 / sizes[i - 1]))\n",
    "            self.biases.append(np.zeros((sizes[i], 1)))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.activations = [X]\n",
    "        self.z = []\n",
    "        for i in range(self.num_layers):\n",
    "            z = np.dot(self.weights[i], self.activations[i]) + self.biases[i]\n",
    "            self.z.append(z)\n",
    "            if i < self.num_layers - 1:\n",
    "                a = np.tanh(z)\n",
    "            else:\n",
    "                a = z  # Linear activation for output\n",
    "            self.activations.append(a)\n",
    "        return self.activations[-1]\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        m = X.shape[1]\n",
    "        gradients = []\n",
    "        dZ = self.activations[-1] - y\n",
    "        for i in range(self.num_layers - 1, -1, -1):\n",
    "            dW = (1 / m) * np.dot(dZ, self.activations[i].T)\n",
    "            db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            gradients.append((dW, db))\n",
    "            if i > 0:\n",
    "                dA = np.dot(self.weights[i].T, dZ)\n",
    "                dZ = dA * (1 - np.tanh(self.z[i - 1]) ** 2)\n",
    "        return gradients[::-1]\n",
    "\n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "        for i in range(self.num_layers):\n",
    "            self.weights[i] -= learning_rate * gradients[i][0]\n",
    "            self.biases[i] -= learning_rate * gradients[i][1]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"../datasets/CropSDEData/METEO_DEKADS_NUTS2_NL.csv\")\n",
    "    df = df.dropna(subset=['TAVG', 'VPRES', 'WSPD', 'RELH', 'PREC'])\n",
    "    \n",
    "    # Extract Heston-estimated parameters\n",
    "    mu_hat = -0.01762635440919763\n",
    "    kappa_hat = 0.05360217296743761\n",
    "    theta_hat = 1.1327667398172898\n",
    "    sigma_hat = 0.2145933527454109\n",
    "    \n",
    "    # Compute log-returns and variance of TAVG\n",
    "    df['log_TAVG'] = np.log(df['TAVG'])\n",
    "    df['returns'] = df['log_TAVG'].diff().dropna()\n",
    "    df['variance'] = df['returns'].rolling(window=30).var().dropna()\n",
    "    df = df.dropna(subset=['returns', 'variance'])\n",
    "    \n",
    "    # Compute Heston variance series\n",
    "    df['heston_variance'] = np.nan\n",
    "    df['heston_variance'].iloc[0] = df['variance'].iloc[0]\n",
    "    np.random.seed(42)\n",
    "    for t in range(1, len(df)):\n",
    "        vt = df['heston_variance'].iloc[t-1]\n",
    "        epsilon = np.random.normal(0, 1)\n",
    "        vt_new = vt + kappa_hat * (theta_hat - vt) + sigma_hat * np.sqrt(vt) * epsilon\n",
    "        df.loc[df.index[t], 'heston_variance'] = max(vt_new, 0)\n",
    "    \n",
    "    # Setup the columns for training and target features\n",
    "    X = df[['VPRES', 'WSPD', 'RELH', 'heston_variance']].values\n",
    "    y = df['TAVG'].values.reshape(-1, 1)\n",
    "    \n",
    "    # Split the data into training and validation datasets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Normalize the input data\n",
    "    X_train_mean = X_train.mean(axis=0)\n",
    "    X_train_std = X_train.std(axis=0)\n",
    "    X_train = (X_train - X_train_mean) / X_train_std\n",
    "    X_val = (X_val - X_train_mean) / X_train_std\n",
    "    \n",
    "    # Define the MLP model\n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_sizes = [64, 64]\n",
    "    output_size = y_train.shape[1]\n",
    "    mlp = MLP(input_size, hidden_sizes, output_size)\n",
    "    \n",
    "    # Training parameters\n",
    "    num_epochs = 5000\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        outputs = mlp.forward(X_train.T)\n",
    "        gradients = mlp.backward(X_train.T, y_train.T)\n",
    "        mlp.update_parameters(gradients, learning_rate)\n",
    "        loss = np.mean((outputs - y_train.T) ** 2)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1} - Loss: {loss}\")\n",
    "        if loss < 0.05:\n",
    "            break\n",
    "    \n",
    "    # Testing\n",
    "    test_outputs = mlp.forward(X_val.T)\n",
    "    test_loss = np.mean((test_outputs - y_val.T) ** 2)\n",
    "    \n",
    "    # Final Metrics\n",
    "    print(\"Final Test MSE:\", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c08d439-509e-461e-873d-b91b10c0b2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
